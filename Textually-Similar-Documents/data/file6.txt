As one can get the idea from the name, “Data Intensive Computing” is the field of computing that deals with large-scale data. What do we mean by large-scale data? Large-scale data can be defined as data that grows to a huge size over a period of time. This can be the data associated with banking transactions or sales data or logs of web servers and many more over a period of time. In this modern era of high-performance computing power available, several application fields ranging from computational science to social networking produce large volumes of daily data that need to be stored efficiently and accessible by everyone within the system. This huge amount of data volumes produced introduces the challenges of how to represent this data using efficient algorithms. how to handle the scalability using distributed computing architectures and algorithms? Data Intensive Computing is the field that exactly addresses these problems and deals with large-scale data in terms of production, manipulation and analysis where the data can range between megabytes (MB) to petabytes (PB). Data Intensive Computing describes how to handle these huge amounts of data using distributed storage systems and data-intensive computing models like MapReduce. Today data-intensive computation occurs in many application domains. From computational science where simulations and experiments produce and analyze huge amounts of data, to several IT giants such as Google, Meta etc that are probably dealing with petabytes of data every single day.